# Yconic Mentor - Architecture Overview

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Next.js Frontend                        â”‚
â”‚                  (Your Teammate Building)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚ HTTP/REST
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FastAPI Server (api.py)                   â”‚
â”‚  Endpoints: /ask, /health, /clear, /reload                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RAG Chain (rag_chain.py)                    â”‚
â”‚  - Conversational retrieval                                  â”‚
â”‚  - System prompts with rubrics                               â”‚
â”‚  - Memory management                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                                   â”‚
           â–¼                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Vector Store        â”‚          â”‚   LLM Wrapper            â”‚
â”‚  (vector_store.py)   â”‚          â”‚   (llm_wrapper.py)       â”‚
â”‚                      â”‚          â”‚                          â”‚
â”‚  - ChromaDB          â”‚          â”‚  Primary: Ollama (local) â”‚
â”‚  - Embeddings        â”‚          â”‚  Fallback: OpenAI        â”‚
â”‚  - Similarity search â”‚          â”‚                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Document Pipeline                             â”‚
â”‚                                                              â”‚
â”‚  S3 Bucket  â†’  S3 Loader  â†’  Text Splitter  â†’  Embeddings  â”‚
â”‚  (s3_loader.py)                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Component Breakdown

### 1. **Document Loading** (`src/loaders/s3_loader.py`)
- Connects to S3 bucket
- Downloads documents (PDFs, DOCX, TXT, etc.)
- Splits into chunks (1000 chars with 200 overlap)
- Preserves metadata (source, type)

**Key Features:**
- Supports directory loading
- Automatic text splitting
- Metadata preservation

### 2. **Vector Store** (`src/embeddings/vector_store.py`)
- Manages ChromaDB vector database
- Handles embeddings (Ollama or OpenAI)
- Provides similarity search
- Persists to disk for reuse

**Key Features:**
- Automatic fallback (Ollama â†’ OpenAI)
- Persistent storage
- Configurable search parameters

### 3. **LLM Wrapper** (`src/llm/llm_wrapper.py`)
- Unified interface for LLMs
- Ollama for local/private inference
- OpenAI as reliable fallback
- Automatic connection testing

**Key Features:**
- Smart fallback mechanism
- Model configuration
- Health checks

### 4. **RAG Chain** (`src/retrieval/rag_chain.py`)
- Conversational retrieval
- Rubrics integration
- Context-aware responses
- Source attribution

**Key Features:**
- Conversation memory
- Custom system prompts
- Source tracking
- Rubrics-based analysis

### 5. **API Server** (`api.py`)
- FastAPI REST endpoints
- CORS for Next.js
- Health checks
- Error handling

**Key Features:**
- OpenAPI docs
- Type validation
- Singleton pattern

### 6. **Main Entry** (`main.py`)
- Orchestrates all components
- Interactive CLI
- Configuration management
- Initialization flow

## ğŸ”„ Request Flow

```
User Question (Next.js)
    â†“
FastAPI /ask endpoint
    â†“
RAG Chain processes question
    â†“
â”œâ”€â†’ Vector Store: Find relevant documents (similarity search)
â”‚   â””â”€â†’ Returns top 6 chunks
â””â”€â†’ LLM: Generate answer with context + rubrics
    â””â”€â†’ Returns answer + sources
        â†“
Response sent to Next.js
```

## ğŸ—‚ï¸ Directory Structure

```
mentor/
â”œâ”€â”€ main.py                    # Main entry point & CLI
â”œâ”€â”€ api.py                     # FastAPI server
â”œâ”€â”€ test_local.py             # Local testing (no S3)
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ QUICKSTART.md            # Hackathon quick start
â”œâ”€â”€ README.md                # Full documentation
â”œâ”€â”€ ARCHITECTURE.md          # This file
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ loaders/
â”‚   â”‚   â””â”€â”€ s3_loader.py     # S3 document loading
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ vector_store.py  # ChromaDB management
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ llm_wrapper.py   # LLM interface
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ rag_chain.py     # RAG implementation
â”‚   â””â”€â”€ rubrics/
â”‚       â””â”€â”€ example_rubrics.json  # Evaluation criteria
â”‚
â”œâ”€â”€ data/                     # Local document cache
â””â”€â”€ chroma_db/               # Vector database storage
```

## ğŸ¯ Design Decisions

### Why ChromaDB?
- âœ… Zero setup (no separate DB server)
- âœ… Persistent storage
- âœ… Great performance for <100K documents
- âœ… Easy to use

### Why LangChain?
- âœ… S3 integration built-in
- âœ… Text splitting utilities
- âœ… Conversational retrieval patterns
- âœ… Works with multiple LLMs

### Why Ollama + OpenAI Fallback?
- âœ… Ollama: Free, private, unlimited
- âœ… OpenAI: Reliable, fast, quality
- âœ… Best of both worlds

### Why FastAPI?
- âœ… Type validation
- âœ… Auto-generated docs
- âœ… Fast development
- âœ… Perfect for Next.js integration

## ğŸ”§ Configuration

### Environment Variables

```env
# LLM Selection
USE_OLLAMA=true              # false for OpenAI only

# Ollama Config
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# OpenAI Config
OPENAI_API_KEY=sk-...

# S3 Config
S3_BUCKET_NAME=your-bucket
S3_DOCUMENTS_PREFIX=documents/
AWS_ACCESS_KEY_ID=...
AWS_SECRET_ACCESS_KEY=...

# System Config
TEMPERATURE=0.3              # Lower = more focused
MAX_TOKENS=2000             # Response length limit
CHROMA_PERSIST_DIRECTORY=./chroma_db
```

### Rubrics Configuration

Edit `src/rubrics/example_rubrics.json` to customize:
- Evaluation categories
- Weights
- Red/green flags
- Criteria

The RAG chain automatically includes rubrics in the system prompt.

## ğŸš€ Scaling Considerations

### Current Limits (Hackathon MVP)
- **Documents**: ~1,000 files (ChromaDB handles well)
- **Concurrent Users**: ~10-20 (single instance)
- **Response Time**: 2-5 seconds

### Future Improvements
1. **Add Pinecone/Weaviate** for production vector storage
2. **Add Redis** for conversation caching
3. **Deploy API** to AWS Lambda/ECS
4. **Add authentication** for multi-user
5. **Stream responses** for better UX
6. **Fine-tune embeddings** for domain specificity

## ğŸ“Š Performance

### Typical Request Latency
```
Document Retrieval:  ~100-200ms  (ChromaDB)
LLM Generation:      ~2-4s       (depends on model)
Total:               ~2-5s
```

### Memory Usage
```
ChromaDB:           ~500MB (for 1000 docs)
Python Process:     ~200-300MB
Ollama (if used):   ~4-8GB GPU RAM
```

## ğŸ”’ Security Notes

- AWS credentials stored in `.env` (never commit!)
- OpenAI keys in `.env` (never commit!)
- S3 bucket should use read-only credentials
- CORS restricted to localhost:3000/3001
- No authentication (add before production!)

## ğŸ§ª Testing

### Unit Tests (Future)
```bash
pytest tests/
```

### Integration Test
```bash
python test_local.py
```

### API Test
```bash
# Start server
python api.py

# In another terminal
curl http://localhost:8000/health
```

## ğŸ“ˆ Monitoring (Production)

Future additions:
- Request logging
- Error tracking (Sentry)
- Performance metrics (Prometheus)
- Cost tracking (OpenAI usage)

---

**Built for Yconic Hackathon 2024**
